{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics vs. Probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability is the study of how (abstract) random variables and distributions behave.  Statistics is the study of how to interpret data while making assumptions about the underlying distributions.  Probability is a rigorous branch of mathematics, involving precise theorems and proofs.  (Applied) statistics is a much more squishy subject that requires imperfect assumptions about real-world phenomena. It is therefore more subject to errors in human judgment and argumentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def plot_hist_dist(rvs, dist, title=None, label='', mean=None, confidence_interval=None, ax=None):\n",
    "    ax = ax if ax else plt.gca()\n",
    "    _, bins, _ = ax.hist(rvs, bins=50, alpha=.6, density=True, label=(label + ' rvs').strip(), color='blue')\n",
    "    xmin, xmax = bins.min(), bins.max()\n",
    "    xpoints = np.arange(xmin, xmax, (xmax - xmin) / 100)\n",
    "    ax.plot(xpoints, dist.pdf(xpoints), label=(label+' pdf').strip(), color='black')\n",
    "    \n",
    "    if mean is not None:\n",
    "        ax.plot([mean, mean], plt.ylim(), label='mean', color='purple')\n",
    "\n",
    "    if confidence_interval:\n",
    "        ymid = np.sum(plt.ylim()) / 2.\n",
    "        plt.text(mean, ymid, 'CI', ha='center', va='bottom')\n",
    "        plt.annotate(\"\", xy=(confidence_interval[0], ymid), xycoords='data',\n",
    "                     xytext=(confidence_interval[1], ymid), textcoords='data',\n",
    "                     arrowprops=dict(arrowstyle=\"|-|\", lw=2, color='r'))    \n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    ax.legend()\n",
    "    \n",
    "def plot_hist_dist_discrete(rvs, dist, title=None, label='', ax=None):\n",
    "    ax = ax if ax else plt.gca()\n",
    "    uniques = np.unique(rvs)\n",
    "    mids = (uniques[1:] + uniques[:-1]) / 2.\n",
    "    bins = np.hstack([[uniques[0]-.5], mids, [uniques[-1] + .5]])\n",
    "    plt.hist(rvs, bins=bins, density=True, label=(label + ' rvs').strip(), alpha=.6, color='blue')\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    plt.plot(uniques, dist.pmf(uniques), label=(label + ' pmf').strip(), color='black')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common applications of statistics are to estimate parameters that describe a population, process, or phenomenon under some model and to test hypotheses (usually about whether a model or model parameters accurately describe a population, process, or phenomenon).\n",
    "\n",
    "Examples of parameter estimation are:\n",
    "- Estimating the mean income of American construction workers\n",
    "- Estimating the variance in daily energy consumption during a summer in New York City\n",
    "- Estimating the correlation coefficient between height and average points per game for power forwards in the NBA\n",
    "\n",
    "Examples of hypothesis testing are:\n",
    "- Testing if there is a significant difference between the mean income of unionized and non-union construction workers\n",
    "- Testing if the variance in energy consumption is significantly larger than the surge capacity of the electrical grid can accommodate\n",
    "- Testing if the correlation coefficient between height and points per NBA game is significantly different from zero\n",
    "\n",
    "Let's examine the example of income of union and non-union construction workers in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in testing whether the mean income of unionized and non-union workers is significantly different is to make measurements. In statistics we call a collection of measurements a **sample**. There are many sampling methods and collecting an unbiased and representative sample is often a difficult task. Sampling theory is beyond the scope of this lecture.\n",
    "\n",
    "Let's say you have a sample of data (e.g. individual unionized worker incomes) $X_1, \\ldots, X_n$.  We estimate the population mean, $\\mu$, as\n",
    "\n",
    "$$ \\overline X = \\frac{1}{n} \\sum_{k=1}^n X_k. $$\n",
    "\n",
    "This is implemented by the function `np.mean()` or the `.mean()` method of a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_empirical_dist(dist, size=100):\n",
    "    X = dist.rvs(size=size)\n",
    "    mean, var = dist.stats('mv')\n",
    "    rvs_mean = X.mean()\n",
    "\n",
    "    plt.hist(X, bins=20, density=True)\n",
    "    ymin, ymax = plt.ylim()\n",
    "    ymid = (ymin + ymax) / 2.\n",
    "    plt.plot([mean]*2, [ymin, ymax], label='true mean')\n",
    "    plt.plot([rvs_mean]*2, [ymin, ymax], label='rvs mean')\n",
    "    plt.text(mean, ymid, 'std', ha='center', va='bottom')\n",
    "    plt.annotate(\"\", xy=(mean-np.sqrt(var), ymid), xycoords='data',\n",
    "                 xytext=(mean+np.sqrt(var), ymid), textcoords='data',\n",
    "                 arrowprops=dict(arrowstyle=\"|-|\", connectionstyle=\"arc3\", lw=2, color='k'))\n",
    "    plt.title(dist.dist.name)\n",
    "    plt.legend()\n",
    "    \n",
    "dists = (\n",
    "    sp.stats.expon(scale=1/2.),\n",
    "    sp.stats.beta(a=2., b=4.),\n",
    "    sp.stats.norm(loc=2., scale=4.),\n",
    "    sp.stats.chi2(df=4.)\n",
    ")\n",
    "\n",
    "for k, dist in enumerate(dists):\n",
    "    plt.subplot(2,2,k+1)\n",
    "    plot_empirical_dist(dist)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a probability distribution that we can use to generate samples of incomes for union construction workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "union_incomes = sp.stats.lognorm(1, loc=20, scale=10)\n",
    "plot_empirical_dist(union_incomes, size=100)\n",
    "plt.plot(np.arange(20, 100, .01), union_incomes.pdf(np.arange(20, 100, .01)), 'k--')\n",
    "plt.xlim([10, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Error of Mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the example code for estimating the mean, we will get different samples and different sample means.\n",
    "\n",
    "As we have seen, an estimate is noisy and is not always equal to its expected value.  What is the uncertainty of our sample mean?  More appropriately, what is the standard deviation of the sampling distribution of the sample mean given our sample size $n$?\n",
    "\n",
    "Let's take our mean estimator $\\overline X$ for the sample $X_1, \\ldots, X_n$.  The Central Limit Theorem tells us that as $n$ increases, our estimate of $\\overline X$ starts to look like the normal distribution:\n",
    "\n",
    "$$ \\overline X \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sample_mean_dist(dist, sample_size, n_samples=100):\n",
    "    sample_means = [dist.rvs(size=sample_size).mean() for i in range(n_samples)]\n",
    "    mean, var = dist.stats('mv')\n",
    "    \n",
    "    plt.hist(sample_means, bins=20, density=True, label='Sample means')\n",
    "    ymin, ymax = plt.ylim()\n",
    "    ymid = (ymin + ymax) / 2.\n",
    "    plt.plot(np.linspace(min(sample_means), \n",
    "                         max(sample_means), 1000), \n",
    "             sp.stats.norm.pdf(np.linspace(min(sample_means), \n",
    "                                           max(sample_means), 1000), \n",
    "                               loc=mean, scale=np.sqrt(var/sample_size)), 'k--',\n",
    "            label = 'Central limit theorem')\n",
    "    plt.title(\"Sampling distribution of sample mean given {} population\".format(dist.dist.name))\n",
    "    plt.legend()\n",
    "    \n",
    "plot_sample_mean_dist(union_incomes, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation of the sampling distribution of the sample mean (i.e. the normal distribution given by the Central Limit Theorem) is called the **standard error** or **standard error of the sample mean**. As we can see from the result of the Central Limit Theorem\n",
    "\n",
    "$$ s_{\\overline X} = \\sqrt{\\frac{\\sigma^2}{n}} $$\n",
    "\n",
    "Given an observation and an underlying distribution, the **z-score** measures the number of standard deviations the observation is above the mean.  For the case of the the observation $\\overline X$, we expect the z-score\n",
    "$$ z = \\frac{\\overline X - \\mu}{\\sigma / \\sqrt{n}} $$\n",
    "to be distributed as a standard normal (with mean zero and standard deviation one).\n",
    "\n",
    "**Gotchas:**\n",
    "- Standard deviation ($\\sigma$ in our case) is not standard error ($\\sigma/\\sqrt{n}$)!  Don't confuse the two.\n",
    "- What is the difference between the standard deviation and the standard error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, var = union_incomes.stats()\n",
    "n=100\n",
    "sampling_hist_std = np.std([union_incomes.rvs(size=n).mean() for i in range(1000)])\n",
    "\n",
    "print(\"Standard error in central limit theorem: {}\".format(np.sqrt(var / n)))\n",
    "print(\"Standard error estimated from repeated sampling: {}\".format(sampling_hist_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we think unionized workers are paid more than their non-union counterparts. We can test this hypothesis in the following conventional framework:\n",
    "\n",
    "1. **Define a null hypothesis, $H_0$.** The null hypothesis is what we assume to be true at the outset. For example, we might naively believe that unions don't affect construction worker pay. If we know that non-union workers make on average \\$32K/yr, then our null hypothesis is that union construction workers mean income is \\$32K/yr. Formally, $H_0$ is $\\mu=32$.\n",
    "  \n",
    "2. **Define an alternative hypothesis, $H_a$ or $H_1$.** The alternative hypothesis is a particular negation of the null hypothesis. In this case, the possibilities are $\\mu > 32$, $\\mu < 32$, or $\\mu \\ne 32$. Because we think unionized workers are paid more than non-union workers, we will choose $\\mu > 32$.\n",
    "\n",
    "3. **Choose a threshold of significance, $\\alpha$.** Assuming that the null hypothesis is true, we should be unlikely to observe samples with mean incomes much higher than \\$32K/yr. The significance level is a probability threshold at which we decide that we could not have observed such a large deviation from the null hypothesis by random chance alone, and that therefore the null hypothesis is false. We'll choose $\\alpha = 0.05$. This is a conventional, _though totally arbitrary_, choice. You should choose an $\\alpha$ based on your _tolerance for error_.\n",
    "\n",
    "We've established that for a sufficiently large sample, the mean income of a sample of union workers is itself a normally distributed random variable with a mean equal to $\\mu$ and variance equal to the square of the standard error. Therefore we can calculate the probability of observing a particular sample mean _or larger_\n",
    "\n",
    "$$ \\mathbb{P}(\\overline X \\ge \\overline x) = 1 - \\int_{-\\infty}^{\\overline x} n(y \\mid \\mu, \\frac{\\sigma^2}{n})dy = 1 - N\\left(\\frac{\\overline x - \\mu}{\\sigma/\\sqrt{n}}\\right)$$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$ \\int_{-\\infty}^{z} n(x \\mid 0, 1)dx = N(z) $$\n",
    "\n",
    "**Question:** How would we calculate the probability of observing a particular sample mean _or less_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, var = union_incomes.stats()\n",
    "h0 = 32\n",
    "\n",
    "n=100\n",
    "sample_mean = union_incomes.rvs(n).mean()\n",
    "x = np.linspace(h0 - 4 * np.sqrt(var / n), h0 + 4 * np.sqrt(var / n), 1000)\n",
    "clt = sp.stats.norm(loc=h0, scale=np.sqrt(var / n))\n",
    "\n",
    "plt.figure(figsize=[16, 4])\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(x, clt.pdf(x), 'k--', label = 'Null hypothesis sampling distribution')\n",
    "plt.fill_between(x[x>sample_mean], 0, clt.pdf(x)[x>sample_mean], label = 'p-value')\n",
    "plt.text(x[-250], clt.pdf(x).max() / 2, 'p-value: {:.3}'.format(1 - clt.cdf(sample_mean)))\n",
    "plt.legend(loc=2)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(x, clt.cdf(x), 'k--', label = 'Null hypothesis sampling distribution')\n",
    "plt.axvline(sample_mean, label='Sample mean')\n",
    "plt.text(sample_mean-4.5, .5, 'p-value: {:.3}'.format(1 - clt.cdf(sample_mean)))\n",
    "plt.legend(loc=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of observing $\\overline X$ *or a result that is more extreme* is called the **p-value**.  When we observe a sample with a p-value less than $\\alpha$, we take it as evidence to reject the null hypothesis.  What does more extreme mean?  Here are two common definitions:\n",
    "1. **One sided test:** If we are testing whether $\\overline X$ is statistically significantly greater than $\\mu$, the p-value would be given by\n",
    "$$1-N\\left(\\overline X \\mid \\mu, \\sigma^2\\right) = 1-N\\left(z \\mid 0, 1\\right)$$\n",
    "where $N(x \\mid \\mu,\\sigma^2)$ is the normal CDF with mean $\\mu$ and standard deviation $\\sigma$.  If we are testing whether $\\overline X$ is statistically significantly less than zero, the p-value would be $N\\left(\\overline X \\mid \\mu, \\sigma^2\\right)$.  As usual, we will probably use $\\sigma = \\hat \\sigma$, the estimated standard deviation.\n",
    "1. **Two sided test:** We are testing whether $\\overline X$ is different (either greater or smaller) than $\\mu$ and if this is statistically significant.  The $p$-value is given by $2 \\cdot N\\left(-\\left|\\overline X\\right|  \\mid  \\mu, \\sigma^2\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals and Probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may decide to reject the null hypothesis, $H_0: \\mu=32$. In that case, what is $\\mu$? Our alternative hypothesis doesn't say!\n",
    "\n",
    "We define the $z$-$\\sigma$ **confidence interval** or $z$-standard-deviation **confidence interval** (where $z \\ge 0$) to be the interval\n",
    "$$ [\\overline X - zs, \\overline X + zs] \\,.$$\n",
    "\n",
    "If we assume the mean estimate is normally distributed (due to the Central Limit Theorem), then we can use the statistics of the normal distribution to compute the probability that the mean estimate falls within the $z$-$\\sigma$ confidence interval.  If $n(x\\mid\\mu,\\sigma)$ is the normal PDF with mean $\\mu$ and standard deviation $\\sigma$, then the probability that the mean falls within the $z$-$\\sigma$ confidence interval is\n",
    "\n",
    "$$ \\int_{\\overline X - zs}^{\\overline X + zs} n(x \\mid \\overline X, s)dx = \\int_{- z}^{z} n(x \\mid 0, 1)dx = N(z) - N(-z) $$\n",
    "where $N$ is the cumulative normal distribution.  We usually choose $z$ to be 2 (~95% confidence interval) or 3 (~99% confidence interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confidence intervals at various rates\n",
    "\n",
    "N = 1000\n",
    "\n",
    "norm = sp.stats.norm()\n",
    "\n",
    "zs = (1, 2, 2.5, 3)\n",
    "\n",
    "for k, z in enumerate(zs):\n",
    "    plt.subplot(2,2,k+1)\n",
    "    plot_hist_dist(norm.rvs(size=N), norm)\n",
    "    plt.plot([-z, -z], [0, norm.pdf(-z)], 'r', label='CI')\n",
    "    plt.plot([z, z], [0, norm.pdf(z)], 'r')\n",
    "    prob = norm.cdf(z) - norm.cdf(-z)\n",
    "    plt.title('{} sigma CI: {:.4f}'.format(z, prob))\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure()\n",
    "z=np.arange(0, 3., .1-1e-9)\n",
    "plt.plot(z, norm.cdf(z) - norm.cdf(-z), color='black')\n",
    "plt.xlabel('z-score')\n",
    "plt.ylabel('probability captured in CI');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing Two Means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our hypothesis testing example, we are interested in whether unionized construction workers make more money than their non-union counterparts. We assert a null hypothesis that they do not, and therefore union workers share the average income of non-union workers: \\$32K/yr. Where did this number come from? We \"magically\" knew it (i.e. we chose it for the example). Sometimes this will be the case -- previous studies will have established good estimates of population parameters that we can then safely assume to be accurate. However, more often we will need to sample both populations that we are comparing. \n",
    "\n",
    "We may collect a sample of incomes, $X_1, \\ldots, X_m$, from union workers and calculate its mean, $\\overline X$, and a sample of incomes from non-union workers, $Y_1, \\ldots, Y_n$, and calculate its mean, $\\overline Y$.  From above, we know that\n",
    "\n",
    "$$ \\overline X \\longrightarrow N\\left(\\mu_X, \\frac{\\sigma^2_X}{m} \\right) \\qquad \\overline Y \\longrightarrow N\\left(\\mu_Y, \\frac{\\sigma^2_Y}{n} \\right)\\,. $$\n",
    "\n",
    "Because we know that the sum of two normal distributions is also normal, we know that\n",
    "\n",
    "$$ \\overline X - \\overline Y \\longrightarrow N\\left(\\mu_X - \\mu_Y, \\frac{\\sigma^2_X}{m} + \\frac{\\sigma^2_Y}{n} \\right)\\,. $$\n",
    "\n",
    "We can use the above to compute a z-score for the difference $\\overline X - \\overline Y$.\n",
    "\n",
    "In this framework we would revise our null hypothesis \n",
    "\n",
    "$$ H_0: \\mu=32 \\Rightarrow H_0: \\mu_x - \\mu_y = 0 $$ \n",
    "\n",
    "**Questions:**\n",
    "1. How do we hypothesis test if $\\overline X > \\overline Y$ for a specific p-value?\n",
    "1. How do the formulas change if we model the underlying variables $X$ and $Y$ as Bernoulli trials?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a collection of data $X_1, \\ldots, X_n$.  To estimate the variance, one might think the right answer is\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{k=1}^n (X_k - \\overline X)^2 $$\n",
    "\n",
    "where $\\overline{X}$ is the mean estimator or the *empirical mean*.  This turns out to be **biased**.  That is, the answer is expected to be smaller than the true variance.  Mathematically, if we let $\\mu$ be the true mean and $\\sigma^2$ be the true variance, then\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{E}\\left[ \\frac{1}{n}\\sum_{k=1}^n \\left(X_k-\\overline{X}\\right)^2 \\right]\n",
    "    &= \\mathbb{E}\\bigg[ \\frac{1}{n}\\sum_{k=1}^n \\big((X_k-\\mu)-(\\overline{X}-\\mu)\\big)^2 \\bigg] \\\\[4pt]\n",
    "    &= \\mathbb{E}\\bigg[ \\frac{1}{n}\\sum_{k=1}^n (X_k-\\mu)^2 -\n",
    "                              2(\\overline{X}-\\mu)(X_k-\\mu) +\n",
    "                              (\\overline{X}-\\mu)^2 \\bigg] \\\\[4pt]\n",
    "    &= \\mathbb{E}\\bigg[ \\frac{1}{n}\\sum_{k=1}^n (X_k-\\mu)^2 - (\\overline{X}-\\mu)^2 \\bigg] \\\\[4pt]\n",
    "    &= \\sigma^2 - \\mathbb{E}\\left[ \\left(\\overline{X}-\\mu\\right)^2 \\right] \\\\\n",
    "    &< \\sigma^2\\,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Mathematically, we can see that\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{E}\\left[ \\left(\\overline{X}-\\mu\\right)^2 \\right]\n",
    "    &= \\mathbb{E}\\left[ \\left(\\frac{1}{n}\\sum_{k=1}^n (X_k-\\mu)\\right)^2 \\right] \\\\[4pt]\n",
    "    &= \\frac{1}{n^2} \\sum_{k=1}^n \\mathbb{E}\\left[ \\left(X_k-\\mu\\right)^2 \\right] \\\\\n",
    "    &= \\frac{\\sigma^2}{n}\n",
    "\\end{align}\\,\n",
    "$$\n",
    "Combining this with the above, we see that \n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{1}{n-1} \\mathbb{E}\\left[ \\sum_{k=1}^n \\left(X_k-\\overline{X}\\right)^2 \\right]\n",
    "    &= \\frac{n}{n-1} \\cdot \\mathbb{E}\\left[ \\frac{1}{n} \\sum_{k=1}^n \\left(X_k-\\overline{X}\\right)^2 \\right] \\\\[4pt]\n",
    "    &= \\frac{n}{n-1} \\cdot \\sigma^2 \\cdot \\left( 1 - \\frac{1}{n} \\right) \\\\[4pt]\n",
    "    &= \\sigma^2 \\,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the **unbiased estimator** of the variance is\n",
    "$$ \\hat\\sigma^2 = \\frac{1}{n-1} \\sum_{k=1}^n (X_k - \\overline X)^2\\, $$\n",
    "\n",
    "Both are implemented by `np.var`.  For the former, set the optional parameter `ddof=0`, the default.  For the latter, set `ddof=1`.  Nominally, the unbiased estimator is assuming a single degree of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = sp.stats.norm()\n",
    "X = dist.rvs(size=[40, 10000])\n",
    "_, var = dist.stats('mv')\n",
    "rvs_var, rvs_var_unbiased  = np.var(X, axis=0), np.var(X, axis=0, ddof=1)\n",
    "\n",
    "plt.hist(rvs_var, label='biased variance', bins=40, alpha=.5)\n",
    "plt.hist(rvs_var_unbiased, label='unbiased variance', bins=40, alpha=.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Calculated variance')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Distribution of biased and unbiased variances\")\n",
    "\n",
    "print(\"True variance to unbiased variance\", np.abs(rvs_var_unbiased.mean() - var))\n",
    "print(\"True variance to biased variance\", np.abs(rvs_var.mean() - var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student t-Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life, we usually do not know $\\sigma$ so we often use $\\hat\\sigma$, the square root of the estimate for the variance $\\sigma^2$.  Because $\\hat\\sigma$ is now a function of the data, even if $\\overline X$ were a normal random variable, the distribution of the z-score\n",
    "\n",
    "$$ z = \\frac{\\overline X - \\mu}{\\hat\\sigma / \\sqrt{n}} $$  \n",
    "\n",
    "is no longer exactly normal but a [**student t**-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) with degrees of freedom $\\nu = n - 1$.  The PDF is given by\n",
    "\n",
    "$$ p(x) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {\\sqrt{\\nu\\pi}\\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{x^2}{\\nu} \\right)^{\\!-\\frac{\\nu+1}{2}} \\,$$\n",
    "\n",
    "The distribution has the following stats:\n",
    "- The mean is $\\mathbb{E}[X] = 0$\n",
    "- The variance is $\\mbox{Var}[X] = \\frac{\\nu}{\\nu-2}$.\n",
    "\n",
    "Fortunately as $\\nu \\to \\infty$ (or $n \\to \\infty$), this approaches the standard normal distribution\n",
    "\n",
    "$$ \\overline X \\longrightarrow N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\,$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and the standard deviation of each of the $X_k$.  So we expect that the **standard error** - the standard deviation of this normal distribution - becomes\n",
    "$$ s = \\frac{\\sigma}{\\sqrt{n}}\\, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large n Assumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption that the distribution of $\\overline X$ is normal is only valid in the limit of large $n$.  This is because\n",
    "1. Central Limit Theorem only ensures that $\\overline X$ becomes normal for large $n$ and\n",
    "1. The use of $\\hat\\sigma$ rather than $\\sigma$ ensures that we are approaching the student t-distribution, which is only approximately normal if $n$ is large.\n",
    "\n",
    "What is the boundary for \"large\"?  Conventional wisdom puts it between 20 and 50. Again, this convention is _totally arbitrary_, and you should decide for yourself based on your _tolerance for error_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Hypothesis Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, instead of testing for just one hypothesis, we end up testing for multiple hypotheses.  This can be very dangerous and is generally something to be avoided.  For example, assume that we are rejecting null hypotheses at 5% p-values.  That is, we reject null hypotheses if, assuming they were true, we would expect to see data this extreme less than 5% of the time.  Then, if we test 20 independent null hypotheses, we'd expect to see sample data that falls outside of a 5% p-value at least once. Hence, we'd expect to (incorrectly) reject 1 null hypothesis even in the event that all 20 null hypotheses were true.\n",
    "\n",
    "There are corrections for this (for example, the [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 [Pragmatic Institute](https://www.pragmaticmarketing.com/data-science). This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
