{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "<!-- requirement: images/linear_regression_error.gif -->\n",
    "<!-- requirement: small_data/cal_house.json.gz -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will cover regression and regression metrics, using Linear Regression as our motivating example.\n",
    "\n",
    "**Regression** is the general term for supervised learning problems where we try to predict the value of a continuous variable.  That is, we want to build a model $f$ that approximates the relationship between features $X$ and labels $y$ so that \n",
    "\n",
    "$$ f(X_j) \\approx y_j $$\n",
    "\n",
    "for each observation $(X_j,y_j)$.  Here the $y_j$ are real numbers, because $y$ is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "In a linear regression model, $f$ is simply a linear function of the features.  If $X_{ji}$ is the $i$th feature of observation $X_j$, then\n",
    "\n",
    "$$ f(X_j) = \\sum_{i=1}^p X_{ji}\\beta_i + \\beta_0 $$\n",
    "\n",
    "where the coefficients $\\beta_i$ and intercept term $\\beta_0$ are values learned by the model during training, and $p$ is the number of features.  This is sometimes written more compactly as\n",
    "\n",
    "$$ f(X) = X \\cdot \\beta + \\beta_0 $$\n",
    "\n",
    "where $X=(X_{ji})$ is a matrix, $\\beta = (\\beta_i)$ is a column vector, and $\\beta_0$ is added elementwise to the result of the product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a simple example.  First we'll generate random data with a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0) # for consistency\n",
    "\n",
    "X = np.linspace(-10,10,30)\n",
    "y = 2 * X + 3 + 4*np.random.randn(X.shape[0])\n",
    "plt.plot(X, y, 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a linear model on this data means finding the coefficient $\\beta$ and intercept $\\beta_0$ so that $f(X) = X\\cdot\\beta+\\beta_0$ best approximates our label variable $y$.  This is equivalent to finding the line $y=mx+b$ that best fits the plotted points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X.reshape(-1,1), y) # reshape to column vector\n",
    "print('Intercept = {:.2f}'.format(lr.intercept_))\n",
    "print('Slope = {:.2f}'.format(lr.coef_[0]))\n",
    "\n",
    "y_pred = lr.predict(X.reshape(-1,1))\n",
    "\n",
    "plt.plot(X, y, 'o', label='training data')\n",
    "plt.plot(X, y_pred, label='model prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "In the preceding discussion, we used phrases like \"best approximates\" and \"best fits\".  In order to actually do machine learning, we need to be more concrete.  That is, we need to define metrics that quantify how well a model fits a given set of data.  Metrics provide a cost function to minimize during training and they act as a benchmark to evaluate trained models.\n",
    "\n",
    "**Mean Squared Error** is the usual metric:\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_j \\left[f(X_{j}) - y_j\\right]^2. $$\n",
    "\n",
    "Unfortunately, this is susceptible to outliers. When this is an issue, **Mean Absolute Error** can be better:\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_j \\left|f(X_{j}) - y_j\\right|. $$\n",
    "\n",
    "You've probably heard of **$R^2$** or the **Coefficient of Determination**. Although it's usually defined in a linear regression context, it's actually a very general idea: it measure the fraction of the error explained by the model $f$ versus the fraction of the error explained by a naive model that assumes the mean value of $y$ (i.e. the variance of $y$):\n",
    "\n",
    "$$ 1 - \\dfrac{\\sum_j \\left[f(X_{j}) - y_j\\right]^2}{\\sum_j \\left(\\overline y - y_j\\right)^2} \\qquad \\mbox{where} \\qquad \\overline y = \\frac{1}{n}\\sum_j y_j \\,.$$\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "1. For a list of scalar values $z_1,\\ldots,z_n$, the **mean** $\\overline z$ is the quantity that minimizes the squared error:\n",
    " $$ \\frac{d}{dz} \\sum_j \\left|z - z_j\\right|^2 = 0$$\n",
    " $$ \\frac{d}{dz} \\left(Nz^2 - 2z(z_1 + z_2 + ...) + z_1^2 + z_2^2 + ...\\right) = 0$$\n",
    " $$ 2Nz - 2(z_1 + z_2 + z_3 + ...) = 0$$\n",
    "\n",
    " $$ z = \\frac{z_1 + z_2 + z_3 + ...}{N} = \\overline z$$\n",
    "\n",
    " Do you know what quantity comes from minimizing the absolute error?\n",
    " $$ \\mbox{argmin}_z \\sum_j \\left|z - z_j\\right| $$\n",
    " Does this help explain why Absolute Error is less susceptible to outliers?\n",
    "1. How does each of these metrics scale as you scale the labels ($y$'s) in our data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are those metrics in scikit learn\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Mean Absolute Error:\", metrics.mean_absolute_error(y, y_pred))\n",
    "print(\"Mean Squared Error:\", metrics.mean_squared_error(y, y_pred))\n",
    "print(\"R^2:\", metrics.r2_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Mean Absolute Error (MAE) and Mean Squared Error (MSE) are both difficult to interpret without context because they depend on the scale of the data.  However, because $R^2$ has a fixed range, a value close to 1 always means that the model is fitting the data fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "As is typical in machine learning setups, training our model is equivalent to solving an optimization problem.  For linear regression, we are trying to find the model parameters $\\beta_i$ which jointly minimize the Squared Error (or equivalently the Mean Squared Error) of our predictions.  In theory we could do this using Gradient Descent, but in practice we take advantage of a direct mathematical solution.\n",
    "\n",
    "To simplify the math, we'll absorb the intercept term $\\beta_0$ into the coefficient vector $\\beta$ by pretending that our feature matrix $X$ has an extra column (a dummy 0th feature) where every value is 1.  Then the linear model can be written as $f(X)=X\\beta$ and our optimization problem is \n",
    "\n",
    "$$\\min_\\beta \\| y - X \\beta \\|^2 $$\n",
    "\n",
    "where $\\| z \\|^2 = \\| z \\|_2^2 = \\sum_i |z_i|^2 $ is the square of the $L^2$ norm.  This problem has a closed form solution given by\n",
    "\n",
    "$$ \\hat \\beta = (X^T X)^{-1} X^T y\\,. $$\n",
    "\n",
    "** Questions: ** \n",
    "\n",
    "1. What about the intercept term?\n",
    "1. Prove that the solution $\\hat \\beta$ actually minimizes the Mean Squared Error.  (Hint: $X (X^T X)^{-1}X^T$ is the projection operator onto the subspace spanned by the columns of $X$).\n",
    "1. What happens if $X^T X$ is singular, e.g. $X$ has two columns that are co-linear.  What does this mean in terms of identification?  When might this occur in the data in real life?\n",
    "1. What happens when $p \\gg n$?  How do you deal with this? ($n$ is the number of observations and $p$ is the number of features, so $X$ is a $n \\times p$ matrix.)\n",
    "1. What is the effect of outliers?  How do you deal with them?\n",
    "1. What if $y$ values are always positive?  What if $y$ values are in a fixed range $[a,b]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent is a optimization method which considers each training observation individually, instead of all at once (as normal gradient descent would). Instead of calculating the exact gradient of the cost function, it uses each observation to estimate the gradient and then takes a step in that direction.  While each individual observation will provide a poor estimate of the true gradient, given enough randomness the parameters will converge to a good global estimate.\n",
    "\n",
    "Because it need only consider a single observation at a time, stochastic gradient descent can handle data sets too large to fit in memory.  Additionally, the training cost is essentially linear in the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "X = X.reshape(-1,1)\n",
    "\n",
    "coefs = []\n",
    "iterations = range(10,1000,100)\n",
    "for n_iter in iterations:\n",
    "    sgd_regressor = linear_model.SGDRegressor(random_state=42, max_iter=n_iter).fit(X, y)\n",
    "    coefs.append(sgd_regressor.coef_)\n",
    "\n",
    "plt.plot(iterations, [c[0] for c in coefs], label=r'$\\beta_1$')\n",
    "plt.legend(loc=3)\n",
    "plt.xlabel('iteration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Features\n",
    "\n",
    "One apparent limitation of linear models is that they cannot capture non-linear behavior.  For example, suppose we try to fit a linear model to data with a quadratic shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-10,10,30)\n",
    "y = -0.8 * X * X + 2 * X + 3 + 8*np.random.randn(X.shape[0])\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X.reshape(-1,1), y) # reshape to column vector\n",
    "\n",
    "y_pred = lr.predict(X.reshape(-1,1))\n",
    "\n",
    "print(\"R^2:\", metrics.r2_score(y, y_pred))\n",
    "\n",
    "plt.plot(X, y, 'o')\n",
    "plt.plot(X, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the model fits the data poorly.  However, if we have a vague idea of what sort of pattern the data follows, then we can help our model by creating artificial features that encode the corresponding nonlinearities.  In this case, it seems that the shape of the data is quadratic, so we can add a feature whose value is always equal to $X^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_quad = np.vstack((X,X**2)).T # Augmented feature matrix with columns X, X**2\n",
    "\n",
    "lr.fit(X_quad, y)\n",
    "\n",
    "y_pred = lr.predict(X_quad)\n",
    "\n",
    "print(\"R^2:\", metrics.r2_score(y, y_pred))\n",
    "\n",
    "plt.plot(X, y, 'o')\n",
    "plt.plot(X, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "As we add complexity to a linear model by increasing the number of features, we also increase the danger of overfitting.  One way to combat this tendency is to add an extra term to our cost function that penalizes model complexity.  We judge model complexity by looking at the sizes of the model coefficients.  One formulation, called **Ridge Regression** uses the combined cost function\n",
    "\n",
    "$$ \\| y - X \\beta \\|^2 + \\alpha \\|\\beta\\|^2 $$\n",
    "\n",
    "where $\\| z \\|^2 = \\sum_i |z_i|^2 $ and $\\alpha$ is a constant (called the **regularization parameter**) chosen by the modeler.  Minimizing this cost function means finding a balance between fitting the training data and keeping model complexity low.  Lower values of $\\alpha$ emphasize the first of these priorities, while higher values emphasize the latter.\n",
    "\n",
    "The penalty term used in Ridge may seem arbitrary at first.  There are alternatives worth considering (for example [Lasso Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)), but it turns out that Ridge can be rigorously motivated on statistical grounds and that the corresponding optimization problem has a closed form solution.  See the reference section at the end of this notebook for more details.  \n",
    "\n",
    "**Questions:**\n",
    "1. What may go wrong if the features do not have comparable scales?  What should you do to prevent this?\n",
    "1. Compared with linear regression, how to you expect the $\\beta$'s to behave?  How does this behavior change as you vary $\\alpha$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example.  If we add more polynomial features to the quadratic model discussed above, then we may grossly overfit the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def extra_features(array_, max_power=20):\n",
    "    '''\n",
    "    Generates higher order polynomial features X, X**2, ... , X**N\n",
    "    Then scales the columns to have mean 0 and standard deviation 1\n",
    "    '''\n",
    "    new_features = np.vstack([array_**i for i in range(1,max_power)]).T\n",
    "    return StandardScaler().fit_transform(new_features)\n",
    "\n",
    "X_fine = np.linspace(-10,10,300) # Additional X values so that we can see prediction behavior between data points\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(extra_features(X),y)\n",
    "y_pred = lr.predict(extra_features(X_fine))\n",
    "\n",
    "plt.ylim(-110,10)\n",
    "plt.plot(X, y, 'o')\n",
    "plt.plot(X_fine, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can address this overfitting by using Ridge regression, implemented as [`sklearn.linear_model.Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_est = Ridge(alpha=0.1)\n",
    "ridge_est.fit(extra_features(X),y)\n",
    "y_pred = ridge_est.predict(extra_features(X_fine))\n",
    "\n",
    "plt.ylim(-110,10)\n",
    "plt.plot(X, y, 'o')\n",
    "plt.plot(X_fine, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: California Housing Data Set\n",
    "\n",
    "Now that we have a thorough understanding of Linear Regression, let's work through a real world example with multiple features.\n",
    "\n",
    "We are going to explore a well-known data set called the *California Housing Data*. This is a useful data set for building and benchmarking models. The data set contains aggregated data on housing values and characteristics by census block. Researchers can study the data to predict the median value of homes based on the other variables.\n",
    "\n",
    "This is a standard data set and comes with Scikit Learn.  However, we have already downloaded it and put it in a json file just in case of connectivity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "cali_data = json.load(gzip.open('small_data/cal_house.json.gz'))\n",
    "\n",
    "X = cali_data['data']\n",
    "y = cali_data['target']\n",
    "print(cali_data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the data to a DataFrame to make it easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, Series\n",
    "\n",
    "names = cali_data['feature_names']\n",
    "\n",
    "data_dict = dict(zip(names, ['Median income', 'House age', 'Average # of rooms', 'Average # of bedrooms', 'Population', 'Average occupancy', 'Latitude', 'Longitude']))\n",
    "\n",
    "cali_df = DataFrame(cali_data['data'], columns=names)\n",
    "\n",
    "home_values = Series(cali_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can familiarize ourselves with the data by plotting it. Let's have some fun exploring data using `IPython` widgets.\n",
    "\n",
    "Experiment with the dropdown to plot each column vs. Median home value.\n",
    "\n",
    "**Question:** Which columns seem to have somewhat of a linear relationship with home values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "def cali_plot(column):\n",
    "    plt.plot(cali_df[column], home_values, '.')\n",
    "    plt.xlabel(data_dict[column])\n",
    "    plt.ylabel('Home Price')\n",
    "\n",
    "dropdown_values = {\"{0}: {1}\".format(k, v):k for k, v in data_dict.items()}\n",
    "\n",
    "widgets.interact(cali_plot, column=dropdown_values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median income feature in particular seems to have a linear relationship with home price. \n",
    "\n",
    "Let's use Linear Regression to find the line that best fits this trend.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = linear_model.LinearRegression(fit_intercept=True)  # fit_intercept=True is the default value\n",
    "linreg.fit(cali_df[['MedInc']], home_values)\n",
    "x = np.linspace(-1, 15).reshape(-1,1)\n",
    "\n",
    "plt.plot(cali_df['MedInc'], home_values, '.')\n",
    "plt.plot(x, linreg.predict(x), '-')\n",
    "plt.ylim(0, 5)\n",
    "plt.xlim(0, 15)\n",
    "plt.xlabel(data_dict['MedInc'])\n",
    "plt.ylabel('Home Price');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the $R^2$ score of this model using the `.score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.score(cali_df[['MedInc']], home_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the model based on median income explains about $47\\%$ of the variance in home prices.\n",
    "\n",
    "Next, let's build a more complicated linear model by using all of the features instead of just median income.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = linear_model.LinearRegression()\n",
    "\n",
    "linreg.fit(cali_df, home_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model learns a coefficient for each feature as well as an intercept term, and then makes predictions by returning the corresponding linear combination.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coefficients: \", linreg.coef_, \"\\n\")\n",
    "print(\"intercept:\", linreg.intercept_, \"\\n\")\n",
    "print((\"prediction = \" +\n",
    "       \"{0} +\\n\".format(linreg.intercept_) +\n",
    "       \" +\\n\".join([\"{1} * {0}\".format(n, f) for n, f in zip(names, linreg.coef_)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the $R^2$ score has improved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.score(cali_df, home_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the new model fits the data better, but that's not necessarily a cause for celebration.\n",
    "\n",
    "* Adding too many features to a model can lead to overfitting.  The fact that our model fits the training data well doesn't necessarily mean that it will generalize to new data.  The proper way to check for this issue is to do a train-test split on the data before training the model (see the overfitting notebook for more details).  Although we omitted this step here for simplicity, you can take our word that this particular model is not overfit.      \n",
    "* Linear models aren't able to make good use of features that don't vary linearly with the target variable.  This means that some features (like house age) don't make a meaningful contribution to the model and that some features (like latitude and longitude) may have non-linear information that's being wasted.  \n",
    "\n",
    "One way to incorporate non-linear information is to create new features.  For example, if you take another look at the plots for latitude and longitude, you'll see that there are two prominent bands in each.  These correspond to the locations of Los Angeles and San Francisco, and we can create new features which reflect the distance from each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def distance_from(cit, lat, lon):\n",
    "    if cit == \"LA\":\n",
    "        lat_0, lon_0 = 34.05, -118.24\n",
    "    if cit == \"SF\":\n",
    "        lat_0, lon_0 = 37.77, -122.42\n",
    "    return np.sqrt((lat-lat_0)**2 + (lon-lon_0)**2)\n",
    "\n",
    "for cit in [\"LA\",\"SF\"]:\n",
    "    cali_df[cit] = distance_from(cit, cali_df[\"Latitude\"], cali_df[\"Longitude\"])\n",
    "    \n",
    "cali_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the minimum of these distances has some correlation with home prices; homes which are further away from a city are cheaper on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_df[\"Closest\"] = cali_df[[\"LA\", \"SF\"]].min(axis=1)\n",
    "\n",
    "plt.plot(cali_df[\"Closest\"], home_values, '.')\n",
    "plt.xlabel(\"Distance To Closest City\")\n",
    "plt.ylabel('Home Price');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do almost as well as our full model from before by using only median income and distance to closest city as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(cali_df[[\"MedInc\", \"Closest\"]], home_values)\n",
    "linreg.score(cali_df[[\"MedInc\", \"Closest\"]], home_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a full model based on our expanded feature set performs marginally better than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = linear_model.LinearRegression()\n",
    "linreg.fit(cali_df, home_values)\n",
    "linreg.score(cali_df, home_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few notes:\n",
    "\n",
    "1. A good baseline is to see how well a mean model performs. That is, take a model that predicts `y.mean()` and whose MSE is going to be `y.var()`.  The mean model always has an $R^2$ of $0$, but that doesn't automatically mean it makes poor predictions.  \n",
    "1. How many (original) features have a correlation coefficient > .6?  These explain the majority of the error (compared with the baseline model).\n",
    "1. One way to prevent this overfitting is to choose only those features $X_{\\cdot i}$ that are highly correlated with $y$.  This can lead much better models.\n",
    "\n",
    "**Question:** \n",
    "1. We tried to predict $y$ but since it is non-negative, it might make sense to predict $\\log(y)$.  What metric would you use to be able to evaluate which one is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: Statistical Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also think of Linear Regression as arising from a statistical model in which the true behavior of data is linear, but each observation has some noise added in the form of an error term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![$L^1$ versus $L^2$ regularization](images/linear_regression_error.gif)\n",
    "\n",
    "Different error distributions give us different classes of the General Linear Models (GLM)s. To learn more about GLMs, there are a good set of notes available [here](http://data.princeton.edu/wws509/notes/a2.pdf).\n",
    "\n",
    "Here we'll assume that the errors are independent and normally distributed with standard deviation $\\sigma$.  Our goal is to use Maximum Likelihood Estimation to estimate the coefficients of the underlying linear function $f(X) = X\\beta$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the coefficient vector $\\beta = (\\beta_i)$ is known, then the probability of measuring $y=(y_j)$ is simply\n",
    "\n",
    "$$ P(y \\mid \\beta) = \\prod_j \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left[-\\left( \\frac{X_j \\cdot \\beta - y_j}{2 \\sigma} \\right)^2 \\right] \\,.$$\n",
    "\n",
    "However, we don't know $\\beta$.  Instead we want to find it, given $y$, by finding the $\\beta$ that maximize $P(\\beta \\mid y)$.  Thanks to Bayes' Rule, we know\n",
    "\n",
    "$$ P(\\beta \\mid y) = P(y \\mid \\beta) \\frac{P(\\beta)}{P(y)} \\,.$$\n",
    "\n",
    "We know the first term on the right hand side, and $P(y)$ is independent of $\\beta$, leaving only $P(\\beta)$ unknown.  In linear regression, we suppose we have no *a priori* knowledge of the expected coefficients and take $P(\\beta)$ to be constant as well.  Thus, the most probable model is determined by maximizing the likelihood function\n",
    "\n",
    "$$ L(\\beta) = \\prod_j \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left[-\\left( \\frac{X_{j} \\cdot \\beta - y_j}{2 \\sigma} \\right)^2 \\right] \\propto P(\\beta \\mid y) \\,.$$\n",
    "\n",
    "Since $\\log$ is monotonic, we can also maximize the log-likelihood.  A few calculations show us that the negative log-likelihood (up to a linear transformation) is\n",
    "\n",
    "$$- \\log(L(\\beta)) \\sim \\| y - X \\beta \\|^2\\,.$$\n",
    "\n",
    "Here, $\\| z \\|^2 = \\| z \\|_2^2 = \\sum_i |z_i|^2 $ is the square of the $L^2$ norm.  The objective is to minimize this quadratic:\n",
    "\n",
    "$$ \\min_\\beta \\| y - X \\beta \\|^2\\,.$$\n",
    "\n",
    "This is the familiar expression for squared error, which means that we've recovered the optimization objective we were using before.  Of course, this means that we can use the same closed form solution:\n",
    "\n",
    "$$ \\hat \\beta = (X^T X)^{-1} X^T y\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to theoretically motivating the use of Mean Squared Error, we can motivate Ridge Regression by revisiting the assumption that $P(\\beta)$ is constant. \n",
    "\n",
    "In the California example, we used an *ad-hoc* criteria to select features.  Essentially, this reflects an expectation that most coefficients should be zero.  A more principled approach is to choose a prior distribution for $P(\\beta_i)$ that is peaked about $\\beta_i = 0$, instead of uniform.  We'll start by taking the coefficients to be identical independently normally distributed about 0 with a standard deviation of $\\sigma/\\sqrt\\alpha$, where $\\alpha$ is a hyperparameter:\n",
    "\n",
    "$$ P (\\beta) \\propto \\prod_i \\exp \\left[ -\\frac{\\alpha}{2} \\left(\\frac{\\beta_i}{\\sigma} \\right)^2\\right] \\,,$$\n",
    "\n",
    "so\n",
    "\n",
    "$$ L(\\beta) \\propto \\prod_j \\exp\\left[ -\\frac{1}{2}\\left( \\frac{X_{j} \\cdot \\beta - y_j}{\\sigma} \\right)^2 \\right]\\prod_i \\exp \\left[ -\\frac{\\alpha}{2} \\left(\\frac{\\beta_i}{\\sigma} \\right)^2\\right] \\,.$$\n",
    "\n",
    "Then the negative log-likelihood is (up to a linear transformation)\n",
    "\n",
    "$$- \\log(L(\\beta)) \\sim \\| y - X \\beta \\|^2 + \\alpha \\| \\beta \\|^2\\,.$$\n",
    "\n",
    "As promised, this is the cost function used in Ridge Regression. The corresponding optimization problem has a closed form solution:\n",
    "\n",
    "$$ \\hat \\beta = (X^T X + \\alpha I)^{-1} X^T y\\,. $$\n",
    "\n",
    "To get some motivation for what's happening, use the *singular value decomposition*\n",
    "\n",
    "$$ X = U \\Sigma V^T $$\n",
    "\n",
    "We  can see that \n",
    "\n",
    "$$ \\hat \\beta = V D U^T y $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ D_{ii} = \\frac{\\Sigma_{ii} }{\\Sigma_{ii}^2 + \\alpha}\\,. $$\n",
    "\n",
    "When $\\alpha = 0$, $D_{ii} = \\frac{1}{\\Sigma_{ii}}$ and it decreases to 0 as $\\alpha \\to \\infty$.  The smaller $\\Sigma_{ii}$, the faster this decrease to 0 (for a given level of $\\alpha$).  So smaller $\\Sigma_{ii}$ are \"shrunk\" faster than larger $\\Sigma_{ii}$ and we get the \"significant values\" are left.\n",
    "\n",
    "**Questions:**\n",
    "1. Can you prove the formula for $\\hat \\beta$ for Ridge Regression from ordinary Linear Regression?\n",
    "1. What is the corresponding prior for plain-vanilla linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How would you assess whether a relationship is actually linear?\n",
    "1. If instead of being able to observe $y$, you observe a noisy estimate of $y \\pm \\epsilon$ with unbiased normally distributed noise.  What is the effect on your estimates $\\beta$?\n",
    "1. When you loaded your data, you unwittingly loaded each row of the data (both $X$ and $y$) twice and performed the same regression.  What is the effect on your estimates $\\beta$?\n",
    "1. When you loaded your data, you unwittingly loaded each column of the features (just $X$) twice and performed the same regression.  What is the effect on your estimates $\\beta$?\n",
    "1. Everything we've talked about so far involves loading all the data into memory.  What if you have more data than you can fit into memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add a column of ones, or subtract off the average value of $y$.\n",
    "1. The hint pretty much gives you the answer since a projection of a point onto a plane is the closest you can get to that point while still remaining on the plane. That being said, you can project the global minimum onto the accessible subspace.  Alternatively, you can differentiate the matrix expression and take the first-order condition and find the zero.\n",
    "1. The matrix can't be inverted.  In practice, this shows up as numerical instabilities.  This will happen if two columns are measuring the same thing, or if one column is a linear combination of two others.\n",
    "1. When $p > n$, X will be degenerate, so it can't be (pseudo-)inverted and you no longer have a unique $\\beta$.  To deal with this, you can reduce the number of features with PCA or use regularization.\n",
    "1. Outliers can really skew the results of $\\beta$ because of the quadratic penalty.  Remember, that minimizing the least squares is essentially looking for a mean, which is affected by outliers.  You can transform the model via quantiles to reduce the effect of noise, bin the data, or use floors and caps on the data.\n",
    "1. For non-negative $y$, try using the $\\log(y)$.  If $y$ is always within a fixed $[a,b]$, use $$\\frac{y - a}{b-a}\\,.$$ Alternatively, scale by the mean / range. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "1. The largest features will be under penalized because the corresponding coefficients are smaller relative to how much they impact the model.  Scale the features beforehand, so that coefficient size gives importance.\n",
    "1. Increasing $\\alpha$ shrinks the terms of $\\beta$ towards zero, with smaller values of $\\beta$ shrunk faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: California Housing Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Look at coefficient of determination; plot residuals and look for a pattern.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: Statistical Motivation\n",
    "\n",
    "1. The formula for $\\hat \\beta$ can be deduced by completing the square.  Then the problem looks exactly like an ordinary Least Squares problem with a different $X$ matrix.\n",
    "1. Recall that the prior is\n",
    "$$\\exp \\left[ -\\left( \\alpha \\frac{\\beta}{2 \\sigma} \\right)^2\\right]$$\n",
    "when $\\alpha = 0$, this is a flat \"improper\" prior (it's not really a distribution).  This is often what a Bayesian calls improper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To assess if the relationship is linear, plot the distribution of the residuals as a function of $x$.  If there's a systematic bias, take a look at it and see what's going on.\n",
    "1. With extra (unbiased) noise, the estimate of $\\beta$ does not change (on average), but the the confidence goes down.\n",
    "1. Loading rows twice has no effect on $\\beta$ but it does artificially increase your confidence (dividing it by a factor $\\sqrt{2}$)\n",
    "1. The problem becomes degenerate and $\\beta_j$ is now split between $\\beta_{j'}$ and $\\beta_{j''}$ such that $\\beta_j = \\beta_{j'} + \\beta_{j''}$.\n",
    "1. All of these problems can be solved using gradient descent, which only requires a *stream* of data, rather than the entire data set.  Linear regression (with either $L^2$, Huber penalty, epsilon insensitive) can be solved using `sklearn.linear_model.SGDRegressor` and logistic regression can be solved using `sklearn.linear_model.SGDClassifier`.  These methods implement a `partial_fit` method, which can iteratively updates the coefficients on small chunks of data.  In this case, you are no longer ram constrained, but constrained in the amount of time it takes to read data from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 [Pragmatic Institute](https://www.pragmaticmarketing.com/data-science). This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
