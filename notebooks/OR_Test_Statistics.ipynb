{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing relies on our ability to calculate the probability of observing a sample like ours. We calculate this probability by calculating a **test statistic** like a z-score or t-score. From there we can calculate the p-value for that test statistic based on its associated distribution. For the z-score, this is the normal distribution, while for the t-score, this is Student's t-distribution.\n",
    "\n",
    "This notebook will discuss how to select the appropriate test statistic based on the hypothesis we want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def plot_hist_dist(rvs, dist, title=None, label='', mean=None, confidence_interval=None, ax=None):\n",
    "    ax = ax if ax else plt.gca()\n",
    "    _, bins, _ = ax.hist(rvs, bins=50, alpha=.6, density=True, label=(label + ' rvs').strip(), color='blue')\n",
    "    xmin, xmax = bins.min(), bins.max()\n",
    "    xpoints = np.arange(xmin, xmax, (xmax - xmin) / 100)\n",
    "    ax.plot(xpoints, dist.pdf(xpoints), label=(label+' pdf').strip(), color='black')\n",
    "    \n",
    "    if mean is not None:\n",
    "        ax.plot([mean, mean], plt.ylim(), label='mean', color='purple')\n",
    "\n",
    "    if confidence_interval:\n",
    "        ymid = np.sum(plt.ylim()) / 2.\n",
    "        plt.text(mean, ymid, 'CI', ha='center', va='bottom')\n",
    "        plt.annotate(\"\", xy=(confidence_interval[0], ymid), xycoords='data',\n",
    "                     xytext=(confidence_interval[1], ymid), textcoords='data',\n",
    "                     arrowprops=dict(arrowstyle=\"|-|\", lw=2, color='r'))    \n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    ax.legend()\n",
    "    \n",
    "def plot_hist_dist_discrete(rvs, dist, title=None, label='', ax=None):\n",
    "    ax = ax if ax else plt.gca()\n",
    "    uniques = np.unique(rvs)\n",
    "    mids = (uniques[1:] + uniques[:-1]) / 2.\n",
    "    bins = np.hstack([[uniques[0]-.5], mids, [uniques[-1] + .5]])\n",
    "    plt.hist(rvs, bins=bins, density=True, label=(label + ' rvs').strip(), alpha=.6, color='blue')\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    plt.plot(uniques, dist.pmf(uniques), label=(label + ' pmf').strip(), color='black')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random variables and sampling distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common examples of hypothesis testing use hypotheses about population means (e.g. the mean income of unionized construction workers is greater than \\$32K/yr). We test the hypothesis by calculating a sample mean\n",
    "\n",
    "$$ \\overline X = \\frac{1}{N}\\sum_i^N X_i $$\n",
    "\n",
    "The sample mean is a sum of random variables, meaning the sample mean itself is a random variable. If the sample mean is a random variable, how is the sample mean distributed (i.e. what kind of random variable is it)?\n",
    "\n",
    "**Short answer:** Because of the Central Limit Theorem, for large N it is approximately normally distributed.\n",
    "**Long answer:** It depends on what kind of random variables the $X_i$ are...\n",
    "\n",
    "More precisely, we are interested in what test statistic (and associated sampling distribution) is appropriate based on our hypothesis and our random variate sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Error of Proportion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes using a normal distribution to approximate a variable does not quite make sense.  For instance, if we are estimating the success rate of a trial with binary outcomes, we can assume the underlying variable $X$ as Bernoulli with a probability of success $p$.  Recall $p$ is also the mean of the Bernoulli so the mean estimator $\\hat p$ is easy to compute.  From probability theory, we know that the variance is given by $\\sigma^2 = p (1-p)$.\n",
    "\n",
    "Even though the proportion estimator $\\hat p$ will be bound between 0 and 1, we will still model the proportion estimator $\\hat p$ as a normal distribution (which is unbounded) with mean $p$ and a standard error\n",
    "$$ s = \\sqrt{\\frac{p (1-p)}{n}}\\,. $$\n",
    "\n",
    "At large $n$ this approximation becomes arbitrarily accurate (the probability outside the 0--1 bounds vanishes), but we know that the probability of observing $k$ successes out of $n$ Bernoulli trials each with probability $p$ of success is given exactly by the binomial distribution. We can therefore calculate the p-value of a sample of Bernoulli random variables using the binomial cumulative distribution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets.widgets import interact\n",
    "\n",
    "def binom_norm_comparison_plot(p=0.9, n=30):\n",
    "    plt.figure(figsize=[14, 4])\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.bar(range(n), sp.stats.binom.pmf(range(n), n, p), label='binomial')\n",
    "    plt.plot(np.linspace(0, n, 1000), sp.stats.norm.pdf(np.linspace(0, n, 1000), n*p, np.sqrt(n*p*(1-p))), 'r', label='normal')\n",
    "    plt.legend()\n",
    "    plt.title('Probability mass/distribution functions')\n",
    "    plt.xlabel('Number of successes')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.bar(range(n), sp.stats.binom.cdf(range(n), n, p), label='binomial')\n",
    "    plt.plot(np.linspace(0, n, 1000), sp.stats.norm.cdf(np.linspace(0, n, 1000), n*p, np.sqrt(n*p*(1-p))), 'r', label='normal')\n",
    "    plt.title('Cumulative distribution functions')\n",
    "    plt.xlabel('Number of successes')\n",
    "    plt.ylabel('Cumulative probability')\n",
    "\n",
    "interact(binom_norm_comparison_plot, p=(0, 1, .1), n=(1, 100, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What would the analogous p-value calculation be if we collected a sample count from a Poisson process (e.g. we counted the number of customers who arrived at a shop in one business day)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule of Three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the confidence interval if we have not observed a single success over $n$ Bernoulli trials?  Naively, $s$ is zero and so the confidence interval seems trivial, but nonsensical.  Rather than doing that, we ask for what values of $p$ is the likelihood of having observed no events greater than or equal to 5%:\n",
    "$$ \\mathbb{P}[X_1 = \\cdots = X_n = 0] \\ge 0.05 $$\n",
    "The left hand side is just $(1-p)^n$, and so taking logs we have\n",
    "$$ -np \\approx n \\ln (1-p) \\ge \\ln(0.05) \\approx -3 $$\n",
    "and so we have\n",
    "$$ p \\lesssim \\frac{3}{n} \\,.$$\n",
    "\n",
    "That is, the 95% confidence interval is approximately $\\left[0, \\frac{3}{n}\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Error of Variance Estimate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Central Limit Theorem tells us that the mean estimator (i.e. a sum of random variables) itself is a random variable that becomes normally distributed as $n$ increases.  How does the variance estimator (or other estimators) behave?  For normally distributed data, the variance estimator (i.e. a sum of squared random variables) behaves as a chi-squared distribution.  The $\\chi^2$ distribution with $n$ degrees of freedom is the distribution given by the sum of $n$ independent standard normal distributions squared:\n",
    "\n",
    "$$ \\chi^2(n) \\sim \\sum_{k=1}^n Z_k^2\\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "norm = sp.stats.norm()\n",
    "sample_sizes = (None, 10, 100, 1000)\n",
    "\n",
    "for k, sample_size in enumerate(sample_sizes):\n",
    "    plt.subplot(2,2,k+1)\n",
    "    if sample_size:\n",
    "        dist = sp.stats.chi2(df=sample_size, scale=1./sample_size)\n",
    "        mean, var = dist.stats(\"mv\")        \n",
    "        plot_hist_dist(\n",
    "            sp.stats.norm().rvs(size=[sample_size, N]).var(axis=0),\n",
    "            dist,\n",
    "            mean=mean,\n",
    "            confidence_interval=[mean - np.sqrt(var), mean + np.sqrt(var)],\n",
    "            title='var est with {:d}'.format(sample_size),\n",
    "        )\n",
    "        plt.xlim([1 - 3./np.log(sample_size), 1 + 3./np.log(sample_size)])\n",
    "    else:\n",
    "        plot_hist_dist(norm.rvs(size=N), norm, title='normal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing for Counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a Poisson process, we can apply a $\\chi^2$ test to check if the counts equal some expected means.  If we hypothesize $H_0$ that $X_1, \\ldots, X_n$ are independent Poisson random variables with expected means $\\lambda_1, \\ldots, \\lambda_n$, we can use this as a basis for hypothesis testing.  Because we know that the mean and variance of $X_k$ is $\\lambda_k$, we know that\n",
    "$$ \\frac{(X_k - \\lambda_k)^2}{\\lambda_k} $$\n",
    "would be a non-negative random variable with expected value 1.  It asymptotically approaches the square of a standard normal.  Therefore, asymptotically,\n",
    "$$ \\sum_{k=1}^n \\frac{(X_k - \\lambda_k)^2}{\\lambda_k} \\sim \\chi^2(n-1)$$\n",
    "\n",
    "Notionally, this is because we are constraining $\\sum_k X_k = \\sum_k \\lambda_k$.\n",
    "We can easily test our hypothesis $H_0$ by testing the p value and rejecting sufficiently extreme p values.\n",
    "\n",
    "**Example:** If we have counts broken up by two variables $X_{ij}$, we can apply the above to study independence.  For example, $i$ may denote whether the person is male or female, and $j$ may denote whether they are left handed, right-handed, or ambidextrous.  Then $X_{ij}$ would denote the number of people of that sex and handedness in a sample population.  We can use the $\\chi^2$ test to study whether sex and handedness are correlated.  If they are not, we expect the levels to be given by\n",
    "$$ \\lambda_{ij} = \\frac{\\hat \\lambda_{i\\cdot} \\hat \\lambda_{\\cdot j}}{\\hat  \\lambda} $$\n",
    "where\n",
    "$$ \\hat \\lambda_{i\\cdot} = \\sum_{j'} X_{i j'} \\qquad \\hat \\lambda_{\\cdot j} = \\sum_{i'} X_{i' j} \\qquad \\hat \\lambda = \\sum_{i'j'} \\lambda_{i'j'}\\,. $$\n",
    "With a little bit of math, we can see that asymptotically\n",
    "$$ \\sum_{k=1}^n \\frac{(X_k - \\lambda_k)^2}{\\lambda_k} \\sim \\chi^2((r-1)(c-1))$$\n",
    "where $r$ is the number of rows (i.e. the number of possible values of $i$; 2 for sex) and $c$ is the number of columns (i.e. the number of possible values of $j$; 3 for handedness)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Where else might we use a chi-squared hypothesis test? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the standard error of an estimate can be difficult.  In general we may not have a suitable analytical approximation for our sampling distribution.  Fortunately we can numerically compute the standard error.  In previous examples, we've numerically computed the error of a parameter estimate by resampling from the entire population.  Sometimes this can be costly, so instead we subsample from the data we have already collected.\n",
    "\n",
    "**Bootstrapping** involves sampling the data (with replacement), computing the estimator on the sample, and using that to infer *nonparametric* estimates of your confidence interval.  Let's suppose the statistic we want to compute on the data $X_1,\\ldots,X_N$ is $\\theta(X_1,\\ldots,X_N)$.  For example, $\\theta$ might be the mean or variance estimators we proposed above.  Next, let's call $\\hat \\theta_k$ the result of applying $\\theta$ on the $k$-th random subsample of the data $X_1,\\ldots,X_N$.  Then the variance of the estimator of $\\theta$ is given by\n",
    "\n",
    "$$ \\frac{1}{B-1} \\sum_{k=1}^B \\left( \\hat \\theta_k - \\frac{1}{B} \\sum_{j=1}^B \\hat \\theta_j\\ \\right)^2\\, $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 400\n",
    "bootstrap_number = 200\n",
    "\n",
    "def bootstrap_samples(rvs, estimator, bootstrap_number):\n",
    "    n = len(rvs)\n",
    "    return np.array([estimator(np.random.choice(rvs, size=n)) for _ in range(bootstrap_number)])\n",
    "\n",
    "dists = (\n",
    "    sp.stats.beta(a=2., b=4.),\n",
    "    sp.stats.chi2(df=10),\n",
    "    sp.stats.expon(),\n",
    "    sp.stats.gamma(a=1.99),\n",
    "    sp.stats.norm(),\n",
    "    sp.stats.uniform(),\n",
    ")\n",
    "\n",
    "for k, dist in enumerate(dists):\n",
    "    plt.subplot(3,2,k+1)\n",
    "    mean, var = dist.stats(\"mv\")\n",
    "    rvs = dist.rvs(size=N)\n",
    "    samples = bootstrap_samples(\n",
    "        rvs,\n",
    "        estimator=lambda x: np.mean(x) - rvs.mean(),\n",
    "        bootstrap_number=bootstrap_number\n",
    "    )\n",
    "    \n",
    "    norm = sp.stats.norm(loc=0., scale=np.sqrt(var / (N-1)))\n",
    "    plot_hist_dist(samples, norm, title=dist.dist.name)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common question is to have data $X_1, \\ldots, X_n$ and wonder whether it has the same distribution as a hypothetical random variable $Y$.  One way to solve this problem is to take the quantile estimate of the points $X_1, \\ldots, X_n$ and compare it to the quantile function $q_Y$ of $Y$.  That is, we plot the sorted points $\\underline X_1, \\ldots, \\underline X_n$ (which form the approximate quantile function) and the points $q_Y(1/n), \\ldots, q_Y(1)$ (or those points of the theoretical quantile function for $Y$).  In other words, we're plotting the points\n",
    "$$ (\\underline X_1, q_Y(1/n)), (\\underline X_2, q_Y(2/n))\\ldots, (\\underline X_n, q_Y(1))\\,. $$\n",
    "Remember that $q_Y(U)$ has the same distribution as $Y$ so if $X$ and $Y$ share the same distribution, then these two sets of points (which are quantile approximations) should conform to the straight line $y=x$.\n",
    "\n",
    "**Practical Usage:**\n",
    "- It turns out that `scipy` will do this for us using the function `probplot`.\n",
    "- It will also gives us the $R^2$ of this fit.  The closer to $1$, the more likely $X$ has the distribution of $Y$.\n",
    "- Finally, don't forget that large quantiles can be noisy, so don't be surprised to see a large amount of variation near the tails.\n",
    "- These are often called Q-Q plots, for quantile-quantile plots.\n",
    "- Q-Q plots are very clear but you can also apply the [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) test, which essentially formulates and automates the testing of Q-Q plots.  This is handled by the function [`scipy.stats.ktest`](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.kstest.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Two-sided distributions and their plots\n",
    "\n",
    "dists = (sp.stats.norm(), sp.stats.t(df=2))\n",
    "\n",
    "for k, distx in enumerate(dists):\n",
    "    for j, disty in enumerate(dists):\n",
    "        plt.subplot(len(dists), len(dists), 1 + len(dists)*k + j)\n",
    "        rvs = distx.rvs(size=1000)\n",
    "        sp.stats.probplot(rvs, dist=disty, plot=plt)\n",
    "        plt.title('{} rvs with {} quantile'.format(distx.dist.name, disty.dist.name))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-sided distributions and their plots\n",
    "\n",
    "dists = (sp.stats.expon(), sp.stats.gamma(6.))\n",
    "\n",
    "for k, distx in enumerate(dists):\n",
    "    for j, disty in enumerate(dists):\n",
    "        plt.subplot(len(dists), len(dists), 1 + len(dists)*k + j)\n",
    "        rvs = distx.rvs(size=1000)\n",
    "        sp.stats.probplot(rvs, dist=disty, plot=plt)\n",
    "        plt.title('{} rvs with {} quantile'.format(distx.dist.name, disty.dist.name))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 [Pragmatic Institute](https://www.pragmaticmarketing.com/data-science). This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
